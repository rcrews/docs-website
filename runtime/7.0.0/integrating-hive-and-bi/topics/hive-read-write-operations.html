<!DOCTYPE html><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="UTF-8"><meta name="DC.Type" content="reference"><meta name="description" content="The API supports reading Spark tables from Hive. HWC supports push-downs of DataFrame filters and projections applied to executeQuery. You can update statements and write DataFrames to partitioned Hive tables, perform batch writes, and use HiveStreaming."><meta name="DC.Relation" scheme="URI" content="../topics/hive_hivewarehousesession_api_operations.html"><meta name="DC.Relation" scheme="URI" content="https://github.com/hortonworks/hive-warehouse-connector-release"><meta name="DC.Relation" scheme="URI" content="http://docs-stage.cloudera.com/HDPDocuments/CR/CR-1.0.0/developing-spark-applications/content/using_spark_hive_warehouse_connector.html"><meta name="DC.Relation" scheme="URI" content="https://issues.apache.org/jira/browse/SPARK-20236"><meta name="prodname" content="Cloudera Runtime"><meta name="version" content="1"><meta name="release" content="0"><meta name="modification" content="0"><meta name="brand" content="Cloudera Runtime"><meta name="rights" content="© 2019 Cloudera, Inc."><meta name="DC.Date.Created" content="2019-04-15"><meta name="DC.Date.Modified" content="2019-04-15"><meta name="DC.Format" content="XHTML"><meta name="DC.Identifier" content="hive-read-write-operations"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="description" content=""><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/5.8.2/css/all.css"><link rel="stylesheet" href="/common/css/main.css"><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/styles/default.min.css"><title>Read and write operations</title></head><body class="hg"><header class="chead"><div class="breadcrumbs">
<span class="bread-home"><a href="/"><i class="fas fa-home"></i><span class="text-home">Cloudera Docs</span></a></span>
<span class="bread-product"></span>
<span class="bread-version"></span>
<span class="bread-category">Category</span></div>
<div class="search"><i class="search-close fas fa-times"></i><input type="text" placeholder="Search Documentation"><i class="fas fa-search"></i></div><div class="launch-search"><i class="fas fa-search"></i></div><div class="launch-pubnav"><i class="fas fa-bars"></i></div></header><main class="cmain"><div class="cpage"><article class="maincontent"><div class="inner-breadcrumbs">Concepts</div><div id="content" aria-labelledby="ariaid-title1">
  <h1 class="title topictitle1" id="ariaid-title1">Read and write operations</h1>
  
  <div class="body refbody"><p class="shortdesc">The API supports reading Spark tables from Hive. HWC supports push-downs of DataFrame filters and projections applied to executeQuery. You can update statements and
    write DataFrames to partitioned Hive tables, perform batch writes, and use
    HiveStreaming.</p>
  <section class="section"><h2 class="title sectiontitle">Read operations</h2>
    
    <p class="p">Execute a Hive SELECT query and return a DataFrame.</p>
    <p class="p"> <code class="ph codeph">hive.executeQuery("select * from web_sales")</code></p>
    
  </section>
  <section class="section"><h2 class="title sectiontitle">Execute a Hive update statement</h2>
    
<p class="p">Execute CREATE, UPDATE, DELETE, INSERT, and MERGE statements in this
  way:</p><p class="p"><code class="ph codeph">hive.executeUpdate("ALTER TABLE old_name RENAME TO
            new_name")</code></p>
  </section>
    <section class="section"><h2 class="title sectiontitle">Write a DataFrame to Hive in batch</h2>
      
      <p class="p">This operation uses LOAD DATA INTO
        TABLE.</p>

        <p class="p">Java/Scala:</p>
        <pre class="pre codeblock"><code>df.write.format(HIVE_WAREHOUSE_CONNECTOR).option("table", &lt;tableName&gt;).save()</code></pre>
        <p class="p">Python:</p>
        <pre class="pre codeblock"><code>df.write.format(HiveWarehouseSession().HIVE_WAREHOUSE_CONNECTOR).option("table", &amp;tableName&gt;).save()</code></pre>
    </section>
      <section class="section"><h2 class="title sectiontitle">Write a DataFrame to Hive, specifying partitions</h2>
        <p class="p">HWC follows Hive semantics for overwriting data with and without partitions and is not
        affected by the setting of <code class="ph codeph">spark.sql.sources.partitionOverwriteMode</code> to
        static or dynamic. This behavior mimics the latest Spark Community trend reflected in
        Spark-20236 (link below).</p>
        <p class="p">Java/Scala:</p><pre class="pre codeblock"><code>df.write.format(HIVE_WAREHOUSE_CONNECTOR).option("table", &lt;tableName&gt;).option("partition", &lt;partition_spec&gt;).save()</code></pre><p class="p">Python:</p><pre class="pre codeblock"><code>df.write.format(HiveWarehouseSession().HIVE_WAREHOUSE_CONNECTOR).option("table", &amp;tableName&gt;).option("partition", &lt;partition_spec&gt;).save()</code></pre>
          <div class="p">Where
          <code class="ph codeph">&lt;partition_spec&gt;</code> is in one of the following forms:<ul class="ul" id="hive-read-write-operations__ul_ycx_tw4_33b">
            <li class="li"><code class="ph codeph">option("partition", "c1='val1',c2=val2") // static</code>
          </li>
            <li class="li"><code class="ph codeph">option("partition", "c1='val1',c2") // static followed by
              dynamic</code></li>
            <li class="li"><code class="ph codeph">option("partition", "c1,c2") // dynamic</code></li>
          </ul></div>
        <div class="p">Depending on the partition spec, HWC generates queries in one of the following forms for
          writing data to Hive. <ul class="ul" id="hive-read-write-operations__ul_dfk_ybp_33b">
            <li class="li">No partitions specified = LOAD DATA</li>
            <li class="li">Only static partitions specified = LOAD DATA...PARTITION</li>
            <li class="li"> Some dynamic partition present = CREATE TEMP TABLE + INSERT INTO/OVERWRITE query.
            </li>
          </ul></div>
        <p class="p">Note: Writing static partitions is faster than writing dynamic partitions.</p>
      </section>
      <section class="section"><h2 class="title sectiontitle">Write a DataFrame to Hive using
        HiveStreaming</h2>
        <p class="p">When using HiveStreaming to write a DataFrame to Hive or a Spark Stream to Hive, you
          need to escape any commas in the stream, as shown in <em class="ph i">Use the Hive Warehouse Connector for Streaming</em> (link below).</p>
        <p class="p">Java/Scala:</p>
        <pre class="pre codeblock"><code>//Using dynamic partitioning
df.write.format(DATAFRAME_TO_STREAM).option("table", &lt;tableName&gt;).save()
          
//Or, writing to a static partition
df.write.format(DATAFRAME_TO_STREAM).option("table", &lt;tableName&gt;).option("partition", &lt;partition&gt;).save() </code></pre>
        <p class="p">Python:</p>
        <pre class="pre codeblock"><code>//Using dynamic partitioning
df.write.format(HiveWarehouseSession().DATAFRAME_TO_STREAM).option("table", &lt;tableName&gt;).save()
          
//Or, writing to a static partition
df.write.format(HiveWarehouseSession().DATAFRAME_TO_STREAM).option("table", &lt;tableName&gt;).option("partition", &lt;partition&gt;).save()     </code></pre>
      </section>
      <section class="section"><h2 class="title sectiontitle">Write a Spark Stream to Hive using HiveStreaming</h2>
        
        
        <p class="p">Java/Scala:</p>
        <pre class="pre codeblock"><code>stream.writeStream.format(STREAM_TO_STREAM).option("table", "web_sales").start()</code></pre>
        <p class="p">Python:</p>
        <pre class="pre codeblock"><code>stream.writeStream.format(HiveWarehouseSession().STREAM_TO_STREAM).option("table", "web_sales").start()</code></pre>
        
      </section>


  
  </div>
<nav role="navigation" class="related-links"><div class="familylinks"><div class="parentlink"><strong>Parent topic:</strong> <a class="link" href="../topics/hive_hivewarehousesession_api_operations.html" title="As a Spark developer, you execute queries to Hive using the JDBC-style HiveWarehouseSession API that supports Scala, Java, and Python. In Spark source code, you create an instance of HiveWarehouseSession. Results are returned as a DataFrame to Spark.">HiveWarehouseSession API operations</a></div></div><div class="linklist relinfo"><strong>Related information</strong><br><div><a class="link" href="https://github.com/hortonworks/hive-warehouse-connector-release" target="_blank">HiveWarehouseConnector Github project (select a feature branch)</a></div><div><a class="link" href="http://docs-stage.cloudera.com/HDPDocuments/CR/CR-1.0.0/developing-spark-applications/content/using_spark_hive_warehouse_connector.html" target="_blank">HiveWarehouseConnector for handling Apache Spark data</a></div><div><a class="link" href="https://issues.apache.org/jira/browse/SPARK-20236" target="_blank">SPARK-20236</a></div></div></nav></div></article><div class="short-prev"><a href="">«</a></div><div class="short-next"><a href="">»</a></div><div class="up"></div><div class="prev"><a href="">« Getting Started with Apache Nifi</a></div><div class="next">Getting Started: <a href="">Terminology Used in This Guide »</a></div></div><aside class="pubmenu"><div class="product-title"><img class="product-logo" src="/common/img/smaller_icons/icon-hdf.png"><span class="product-name">Cloudera Data Platform</span></div><nav class="ctoc"></nav></aside></main><div class="logo"><a href="/"><img src="//www.cloudera.com/apps/settings/wcm/designs/www/clientlibs/css/assets/icons/favicon/apple-touch-icon-152x152.png"></a></div><nav class="product-drawer"><div class="full-logo"><img src="/common/img/cloudera.png"></div><ul class="products"><li class="cat">Data Platforms</li><li><img src="/common/img/mini_icons/icon-ambari.png"><span class="text">Ambari</span></li><li><img src="/common/img/mini_icons/icon-hdp.png"><span class="text">Data Platform</span></li><li class="active"><img src="/common/img/mini_icons/icon-hdf.png"><span class="text">Dataflow</span></li><li><img src="/common/img/mini_icons/icon-smartsense.png"><span class="text">Smartsense</span></li><li><img src="/common/img/mini_icons/icon-cybersecurity.png"><span class="text">Cybersecurity</span></li><li class="cat">Data Services</li><li><img src="/common/img/mini_icons/icon-dataplane.png"><span class="text">DataPlane Core</span></li><li><img src="/common/img/mini_icons/icon-studio.png"><span class="text">Data Analytics Studio</span></li><li><img src="/common/img/mini_icons/icon-datasteward.png"><span class="text">Data Steward Studio</span></li><li class="cat">Cloud Services</li><li><img src="/common/img/mini_icons/icon-cloudbreak.png"><span class="text">Cloudbreak</span></li><li><img src="/common/img/mini_icons/icon-hdcloud.png"><span class="text">HDCloud for AWS</span></li></ul><div class="open-close">»</div></nav><footer></footer><script src="//cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script><script src="/common/js/main.js"></script><script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/highlight.min.js" class="test"></script></body></html>