<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="UTF-8"><meta name="DC.Type" content="reference"><meta name="description" content="HiveWarehouseSession acts as an API to bridge Spark with Hive. In your Spark source code, you create an instance of HiveWarehouseSession. You use the language-specific code to create the HiveWarehouseSession."><meta name="DC.subject" content="Hive, Spark, Hive Warehouse Connector, HiveWarehouseConnector"><meta name="keywords" content="Hive, Spark, Hive Warehouse Connector, HiveWarehouseConnector"><meta name="DC.Relation" scheme="URI" content="../topics/cr-hive_hivewarehouseconnector_for_handling_apache_spark_data.html"><meta name="DC.Relation" scheme="URI" content="https://github.com/hortonworks/hive-warehouse-connector-release"><meta name="DC.Relation" scheme="URI" content="http://docs-dev.cloudera.com.s3.amazonaws.com/HDPDocuments/CR/CR-master/developing-spark-applications/content/using_spark_hive_warehouse_connector.html"><meta name="DC.Relation" scheme="URI" content="https://community.hortonworks.com/content/kbentry/223626/integrating-apache-hive-with-apache-spark-hive-war.html"><meta name="DC.Relation" scheme="URI" content="https://hortonworks.com/blog/hive-warehouse-connector-use-cases/"><meta name="prodname" content="Cloudera Runtime"><meta name="version" content="1"><meta name="release" content="0"><meta name="modification" content="0"><meta name="brand" content="Cloudera Runtime"><meta name="rights" content="© 2019 Cloudera, Inc."><meta name="DC.Date.Created" content="2019-04-15"><meta name="DC.Date.Modified" content="2019-04-15"><meta name="DC.Format" content="XHTML"><meta name="DC.Identifier" content="hive_hivewarehousesession_api_operations"><link rel="stylesheet" type="text/css" href="../commonltr.css"><title>HiveWarehouseSession API operations</title><script type="application/javascript">
      
      
    //The id for tree cookie
    var treeCookieId = "treeview-bk_release-notes";
    var language = "en";
    var w = new Object();
    //Localization
    txt_filesfound = 'Results';
    txt_enter_at_least_1_char = "You must enter at least one character.";
    txt_browser_not_supported = "Please enable JavaScript.";
    txt_please_wait = "Please wait. Search in progress...";
    txt_results_for = "Results for: ";
    
    
    </script><script type="application/javascript" src="https://code.jquery.com/jquery-1.4.3.min.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/jquery-ui-1.8.2.custom.min.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/jquery.cookie.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/jquery.treeview.min.js"></script><link href="https://cdn.jsdelivr.net/qtip2/3.0.3/jquery.qtip.min.css" rel="stylesheet" type="text/css" media="screen, projection"><script src="https://cdnjs.cloudflare.com/ajax/libs/qtip2/3.0.3/jquery.qtip.min.js" type="application/javascript"></script><script type="application/javascript" src="/common/js/navigation.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/main.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/htmlFileList.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/htmlFileInfoList.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/nwSearchFnt.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/en_stemmer.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/index-1.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/index-2.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/index-3.js"></script><script type="application/javascript">
      
      
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-22950817-19', 'auto');
    ga('send', 'pageview');
    
    
    </script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/plugins.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/scripts.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/highlight.min.js" type="application/javascript"></script><script type="application/javascript">
      
    // PAGE SPECIFIC SCRIPTS
    
    $('.chapters>li .dropdown').click(function () {
    var $arrow = $(this).find('i.fa');
    if( $arrow.hasClass('fa-chevron-down')) {
    $(this).siblings('.sections').show();
    $arrow.removeClass('fa-chevron-down').addClass('fa-chevron-up');
    } else {
    console.log('up');
    $(this).siblings('.sections').hide();
    $arrow.removeClass('fa-chevron-up').addClass('fa-chevron-down');
    }
    });
    
    //  $('h3').on('mouseenter',function() {
    //      console.log('received mouseover');
    //      $(this).find('.hash-link').show();
    //  });
    
    //  $('h3').on('mouseleave',function() {
    //      $(this).find('.hash-link').hide();
    //  });
    
    
    // STICKY SIDEBARs
    var stickySidebar = $('.sticky');
    
    if (stickySidebar.length > 0) {
    var stickyHeight = stickySidebar.height(),
    sidebarTop = stickySidebar.offset().top;
    }
    
    // on scroll move the sidebar
    $(window).scroll(function () {
    if (stickySidebar.length > 0) {
    var scrollTop = $(window).scrollTop();
    console.log
    
    if (sidebarTop < scrollTop) {
    stickySidebar.css('top', scrollTop - sidebarTop);
    
    // stop the sticky sidebar at the footer to avoid overlapping
    var sidebarBottom = stickySidebar.offset().top + stickyHeight,
    stickyStop = $('.main-content').offset().top + $('.main-content').height();
    if (stickyStop < sidebarBottom) {
    var stopPosition = $('.main-content').height() - stickyHeight;
    stickySidebar.css('top', stopPosition);
    }
    }
    else {
    stickySidebar.css('top', '0');
    }
    }
    });
    
    $(window).resize(function () {
    if (stickySidebar.length > 0) {
    stickyHeight = stickySidebar.height();
    }
    });
    
    // Initialize Syntax Highlighting
    hljs.initHighlightingOnLoad();
    
    </script><style type="text/css">
      
      
    input {
    margin-bottom: 5px;
    margin-top: 2px;
    }
    
    .folder {
    display: block;
    height: 22px;
    padding-left: 20px;
    background: transparent url(/common/themes/pre-hdp-3.1/images/folder.gif) 0 0px no-repeat;
    }
    
    </style><link href="/common/themes/pre-hdp-3.1/images/favicon.ico" rel="icon" type="image/x-icon" sizes="16x16"><link href="https://hortonworks.com/wp-content/uploads/2016/04/cropped-favicon521x512-70x70.png" rel="icon" type="image/x-icon" sizes="32x32"><link href="https://hortonworks.com/wp-content/uploads/2016/04/cropped-favicon521x512-300x300.png" rel="icon" type="image/x-icon" sizes="192x192"><link href="https://hortonworks.com/wp-content/uploads/2016/04/cropped-favicon521x512-300x300.png" rel="apple-touch-icon-precomposed" type="image/x-icon" sizes="192x192"><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/styles/default.min.css" rel="stylesheet"><link rel="stylesheet" media="screen, projection" href="/common/themes/pre-hdp-3.1/css/main.css"><link rel="stylesheet" media="screen, projection" href="/common/themes/pre-hdp-3.1/css/docux.css"><style type="text/css"></style><link rel="stylesheet" type="text/css" media="screen, projection" href="/common/themes/pre-hdp-3.1/css/jquery-ui-1.8.2.custom.css"><link rel="stylesheet" type="text/css" media="screen, projection" href="/common/themes/pre-hdp-3.1/css/jquery.treeview.css"><link href="https://cdn.jsdelivr.net/qtip2/3.0.3/jquery.qtip.min.css" rel="stylesheet" type="text/css" media="screen, projection"></head><body class="docs books"><div id="body_container"><div class="breadcrumb_bg"><!-- --></div><div id="navbar"><section class="ps topstrip"><div><!-- --></div></section></div><div id="mainbody"><section class="ps titlebar"><div><div class="breadcrumb"><a href="/">Hortonworks Docs</a> » <a id="index_page" href="../../index.html">Cloudera Runtime 1.0.0</a> » <span class="current_book">Integrating Apache Hive with Spark and BI</span></div><div class="book_title">Integrating Apache Hive with Spark and BI</div><div class="ebooks"><div class="formats"><span class="desc">Also available as:</span><div class="format"><a onclick="_gaq.push(['_trackEvent', 'Header', 'pdfDownload', 'click', 1]);" title="Download a PDF of this document" class="pdficon" href="../cr-hive_integrating_hive_and_bi.pdf"><img src="/common/themes/pre-hdp-3.1/images/pdf.png" alt="PDF"></a></div></div></div></div></section><div class="lowerbody"><section class="ps doc-content"><div><div class="aside left"><div><div id="leftnavigation" style="padding-top:3px; background-color:white;"><div class="ui-tabs ui-widget ui-widget-content ui-corner-all"><div id="treeDiv" class="ui-tabs-panel ui-widget-content ui-corner-bottom"><img src="/common/themes/pre-hdp-3.1/images/loading.gif" alt="loading table of contents..." id="tocLoading" style="display:block;"><div id="ulTreeDiv" style="display:none" class="thisisthat"><ul id="tree" class="filetree"><li><a href="../topics/cr-hive_hivewarehouseconnector_for_handling_apache_spark_data.html">Hive Warehouse Connector for accessing Apache Spark data</a><ul><li><a href="../topics/cr-hive_apache_spark_hive_connection_configuration.html">Apache Spark-Apache Hive connection configuration</a></li><li><a href="../topics/cr-hive_submit_a_hivewarehouseconnector_app.html">Submit a Hive Warehouse Connector Scala or Java application</a></li><li><a href="../topics/cr-hive_submit_a_hivewarehouseconnector_python.html">Submit a Hive Warehouse Connector Python app</a></li><li><a href="../topics/cr-hive_hivewarehouseconnector_supported_types.html">Hive Warehouse Connector supported types</a></li><li class="active"><a href="../topics/cr-hive_hivewarehousesession_api_operations.html">HiveWarehouseSession API operations</a></li><li><a href="../topics/cr-hive_use_hwc_for_streaming.html">Use the Hive Warehouse Connector for streaming</a></li></ul></li><li><a href="../topics/cr-hive_query_sql_using_jdbcstoragehandler.html">Query a SQL data source using the JdbcStorageHandler</a></li></ul></div></div></div></div></div><div class="formats"><div class="pdf"><!-- --></div></div></div><div class="next-prev-links"></div><div id="content" aria-labelledby="ariaid-title1">
  <h1 class="title topictitle1" id="ariaid-title1">HiveWarehouseSession API operations</h1>
  
  
  <div class="body refbody"><p class="shortdesc">HiveWarehouseSession acts as an API to bridge Spark with Hive. In your Spark source code, you create an instance of HiveWarehouseSession. You use the language-specific code to create the HiveWarehouseSession.</p>
    <section class="section"><h2 class="title sectiontitle">Import statements and variables</h2>
      
      <p class="p">The following string constants are defined by the API:</p>
      <ul class="ul">
        <li class="li"><code class="ph codeph">HIVE_WAREHOUSE_CONNECTOR</code></li>
        <li class="li"><code class="ph codeph">DATAFRAME_TO_STREAM</code></li>
        <li class="li"><code class="ph codeph">STREAM_TO_STREAM</code></li>
        </ul>
      <p class="p">For more information, see the Github project for the Hive Warehouse Connector.</p>
      <p class="p">Assuming <code class="ph codeph">spark</code> is running in an existing <code class="ph codeph">SparkSession</code>,
        use this code for imports:</p>
      <ul class="ul">
        <li class="li">Scala<div class="p">
            <pre class="pre codeblock"><code>import com.hortonworks.hwc.HiveWarehouseSession
import com.hortonworks.hwc.HiveWarehouseSession._
val hive = HiveWarehouseSession.session(spark).build()</code></pre>
          </div></li>
      </ul>
      <ul class="ul">
        <li class="li">Java<div class="p">
            <pre class="pre codeblock"><code>import com.hortonworks.hwc.HiveWarehouseSession;
import static com.hortonworks.hwc.HiveWarehouseSession.*;
HiveWarehouseSession hive = HiveWarehouseSession.session(spark).build();</code></pre>
          </div></li>
      </ul>
      <ul class="ul">
        <li class="li">Python<div class="p">
            <pre class="pre codeblock"><code>from pyspark_llap import HiveWarehouseSession
hive = HiveWarehouseSession.session(spark).build()</code></pre>
          </div></li>
      </ul>
    </section>
    <section class="section"><h2 class="title sectiontitle">Catalog operations</h2>
      
      <ul class="ul">
      <li class="li"><p class="p">Set the current database for unqualified Hive table references</p>
         <p class="p"> <code class="ph codeph">hive.setDatabase(&lt;database&gt;)</code></p></li>
        <li class="li"><p class="p">Execute a catalog operation and return a DataFrame</p>
          <p class="p"><code class="ph codeph">hive.execute("describe extended web_sales").show(100)</code></p></li>
        <li class="li"><p class="p">Show databases</p>
          <p class="p"><code class="ph codeph">hive.showDatabases().show(100)</code></p></li>
       <li class="li"><p class="p">Show tables for the current database</p>
         <p class="p"><code class="ph codeph">hive.showTables().show(100)</code></p></li>
       <li class="li"> <p class="p">Describe a table</p>
         <p class="p"><code class="ph codeph">hive.describeTable(&lt;table_name&gt;).show(100)</code></p></li>
       <li class="li"> <p class="p">Create a database</p>
          <p class="p"><code class="ph codeph">hive.createDatabase(&lt;database_name&gt;,&lt;ifNotExists&gt;)</code></p></li>
        <li class="li"><p class="p">Create an ORC table</p>
         
            <pre class="pre codeblock"><code>hive.createTable("web_sales").ifNotExists().column("sold_time_sk", "bigint").column("ws_ship_date_sk", "bigint").create()</code></pre>
            <p class="p">See the CreateTableBuilder interface section below for additional table creation
              options. Note: You can also create tables through standard HiveQL using
                <code class="ph codeph">hive.executeUpdate</code>.</p></li>
          
     <li class="li"> <p class="p">Drop a database</p>
       <p class="p"><code class="ph codeph">hive.dropDatabase(&lt;databaseName&gt;, &lt;ifExists&gt;,
            &lt;useCascade&gt;)</code></p></li>
       <li class="li"><p class="p">Drop a table</p>
          <p class="p"><code class="ph codeph">hive.dropTable(&lt;tableName&gt;, &lt;ifExists&gt;, &lt;usePurge&gt;)</code></p></li>
      </ul>
    </section>
    <section class="section"><h2 class="title sectiontitle">Read operations</h2>
      
      <p class="p">Execute a Hive SELECT query and return a DataFrame.</p>
         <p class="p"> <code class="ph codeph">hive.executeQuery("select * from web_sales")</code></p>
        
    </section>
    <section class="section"><h2 class="title sectiontitle">Write operations</h2>
      
      <ul class="ul">
    <li class="li"> <p class="p">Execute a Hive update statement</p>
      <p class="p"><code class="ph codeph">hive.executeUpdate("ALTER TABLE old_name RENAME TO new_name")</code></p><p class="p">Note:
        You can execute CREATE, UPDATE, DELETE, INSERT, and MERGE statements in this
        way.</p></li>
        <li class="li"><p class="p">Write a DataFrame to Hive in batch (uses LOAD DATA INTO
            TABLE)</p>
            <p class="p">Java/Scala:</p>
            <pre class="pre codeblock"><code>df.write.format(HIVE_WAREHOUSE_CONNECTOR).option("table", &lt;tableName&gt;).save()</code></pre>
          <p class="p">Python:</p>
            <pre class="pre codeblock"><code>df.write.format(HiveWarehouseSession().HIVE_WAREHOUSE_CONNECTOR).option("table", &amp;tableName&gt;).save()</code></pre>
      </li>
         <li class="li"><p class="p">Write a DataFrame to Hive using
            HiveStreaming</p>
          
            <p class="p">Java/Scala:</p>
            <pre class="pre codeblock"><code>//Using dynamic partitioning
df.write.format(DATAFRAME_TO_STREAM).option("table", &lt;tableName&gt;).save()

//Or, to write to static partition
df.write.format(DATAFRAME_TO_STREAM).option("table", &lt;tableName&gt;).option("partition", &lt;partition&gt;).save()
            </code></pre>
            <p class="p">Python:</p>
          <pre class="pre codeblock"><code>//Using dynamic partitioning
df.write.format(HiveWarehouseSession().DATAFRAME_TO_STREAM).option("table", &lt;tableName&gt;).save()
              
//Or, to write to static partition
df.write.format(HiveWarehouseSession().DATAFRAME_TO_STREAM).option("table", &lt;tableName&gt;).option("partition", &lt;partition&gt;).save()
          </code></pre>
         </li>
      <li class="li"><p class="p">Write a Spark Stream to Hive using HiveStreaming.</p>
          
         
            <p class="p">Java/Scala:</p>
            <pre class="pre codeblock"><code>stream.writeStream.format(STREAM_TO_STREAM).option("table", "web_sales").start()</code></pre>
        <p class="p">Python:</p>
            <pre class="pre codeblock"><code>stream.writeStream.format(HiveWarehouseSession().STREAM_TO_STREAM).option("table", "web_sales").start()</code></pre>
          
       </li>
      </ul>
    </section>
    <section class="section"><h2 class="title sectiontitle">Write a DataFrame from Spark to Hive example</h2>
      
      <p class="p">You can create the DataFrame from any data source and include an option to write the
        DataFrame to a Hive table. When you write the DataFrame, the Hive Warehouse Connector
        creates the Hive table if it does not exist. You should specify a <a class="xref" href="https://spark.apache.org/docs/2.3.1/api/java/org/apache/spark/sql/SaveMode.html" target="_blank">Spark <code class="ph codeph">SaveMode</code></a>, such as Append.</p>
      <pre class="pre codeblock"><code>df = //Create DataFrame from any source
        
val hive = com.hortonworks.spark.sql.hive.llap.HiveWarehouseBuilder.session(spark).build()
        
df.write.format(HIVE_WAREHOUSE_CONNECTOR)
.mode("append")
.option("table", "my_Table")
.save()     </code></pre>
    </section>
    <section class="section"><h2 class="title sectiontitle">ETL example (Scala)</h2>
     
     <p class="p">Read table data from Hive, transform it in Spark, and write to a new Hive table.</p>
     <pre class="pre codeblock"><code>import com.hortonworks.hwc.HiveWarehouseSession
import com.hortonworks.hwc.HiveWarehouseSession._
val hive = HiveWarehouseSession.session(spark).build()
hive.setDatabase("tpcds_bin_partitioned_orc_1000")
val df = hive.executeQuery("select * from web_sales")
df.createOrReplaceTempView("web_sales")
hive.setDatabase("testDatabase")
hive.createTable("newTable")
  .ifNotExists()
  .column("ws_sold_time_sk", "bigint")
  .column("ws_ship_date_sk", "bigint")
  .create()
sql("SELECT ws_sold_time_sk, ws_ship_date_sk FROM web_sales WHERE ws_sold_time_sk &gt; 80000)
  .write.format(HIVE_WAREHOUSE_CONNECTOR)
  .mode("append")
  .option("table", "newTable")
  .save()</code></pre>
   </section>
    <section class="section"><h2 class="title sectiontitle">HiveWarehouseSession interface</h2>
      
      <pre class="pre codeblock"><code>package com.hortonworks.hwc;

public interface HiveWarehouseSession {

//Execute Hive SELECT query and return DataFrame
	  Dataset&lt;Row&gt; executeQuery(String sql);

//Execute Hive update statement
	  boolean executeUpdate(String sql);

//Execute Hive catalog-browsing operation and return DataFrame
	  Dataset&lt;Row&gt; execute(String sql);

//Reference a Hive table as a DataFrame
	  Dataset&lt;Row&gt; table(String sql);

//Return the SparkSession attached to this HiveWarehouseSession
	  SparkSession session();

//Set the current database for unqualified Hive table references
	  void setDatabase(String name);

/**
 * Helpers: wrapper functions over execute or executeUpdate
*/

//Helper for show databases
	  Dataset&lt;Row&gt; showDatabases();

//Helper for show tables
	  Dataset&lt;Row&gt; showTables();

//Helper for describeTable
	  Dataset&lt;Row&gt; describeTable(String table);

//Helper for create database
	  void createDatabase(String database, boolean ifNotExists);

//Helper for create table stored as ORC
	  CreateTableBuilder createTable(String tableName);

//Helper for drop database
	  void dropDatabase(String database, boolean ifExists, boolean cascade);

//Helper for drop table
	  void dropTable(String table, boolean ifExists, boolean purge);
}</code></pre>
    </section>
    <section class="section"><h2 class="title sectiontitle">CreateTableBuilder interface</h2>
      
      <div class="p"><pre class="pre codeblock"><code>
package com.hortonworks.hwc;
        
public interface CreateTableBuilder {
        
  //Silently skip table creation if table name exists
  CreateTableBuilder ifNotExists();
        
  //Add a column with the specific name and Hive type
  //Use more than once to add multiple columns
  CreateTableBuilder column(String name, String type);
        
  //Specific a column as table partition
  //Use more than once to specify multiple partitions
  CreateTableBuilder partition(String name, String type);
        
  //Add a table property
  //Use more than once to add multiple properties
  CreateTableBuilder prop(String key, String value);
        
  //Make table bucketed, with given number of buckets and bucket columns
  CreateTableBuilder clusterBy(long numBuckets, String ... columns);
        
  //Creates ORC table in Hive from builder instance
  void create();
}   </code></pre></div>
    </section>
  </div>
<nav role="navigation" class="related-links"><div class="familylinks"><div class="parentlink"><strong>Parent topic:</strong> <a class="link" href="../topics/cr-hive_hivewarehouseconnector_for_handling_apache_spark_data.html" title="The Hive Warehouse Connector (HWC) is a Spark library/plugin that is launched with the Spark app. You need to understand how to use HWC to access Spark tables from Hive. You also export tables to Hive from Spark and vice versa using this connector.">Hive Warehouse Connector for accessing Apache Spark data</a></div></div><div class="linklist relinfo"><strong>Related information</strong><br><div><a class="link" href="https://github.com/hortonworks/hive-warehouse-connector-release" target="_blank">HiveWarehouseConnector Github project (select a feature branch)</a></div><div><a class="link" href="http://docs-dev.cloudera.com.s3.amazonaws.com/HDPDocuments/CR/CR-master/developing-spark-applications/content/using_spark_hive_warehouse_connector.html" target="_blank">HiveWarehouseConnector for handling Apache Spark data</a></div><div><a class="link" href="https://community.hortonworks.com/content/kbentry/223626/integrating-apache-hive-with-apache-spark-hive-war.html" target="_blank">Hortonworks Community Connection: Integrating Apache Hive with Apache Spark-\-Hive Warehouse Connector</a></div><div><a class="link" href="https://hortonworks.com/blog/hive-warehouse-connector-use-cases/" target="_blank">Hive Warehouse Connector Use Cases</a></div></div></nav></div></div></section></div></div></div><section class="ps footer"><div><div class="copyright">© 2012–2019, Hortonworks, Inc.</div><div class="license">Document licensed under the <a href="https://creativecommons.org/licenses/by-sa/4.0/legalcode">Creative
          Commons Attribution ShareAlike 4.0 License</a>.</div><div class="menulinks"><a href="https://hortonworks.com">Hortonworks.com</a> | 
          <a href="#">Documentation</a> |
          <a href="https://hortonworks.com/support/">Support</a> |
          <a href="https://community.hortonworks.com/">Community</a></div></div></section></body></html>