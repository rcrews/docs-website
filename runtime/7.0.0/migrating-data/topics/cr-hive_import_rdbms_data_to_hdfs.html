<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="UTF-8"><meta name="DC.Type" content="task"><meta name="description" content="You create a single Sqoop import command that imports data from diverse data sources, such as a relational database, into HDFS."><meta name="DC.Relation" scheme="URI" content="../topics/cr-hive_moving_data_from_hdfs_to_hive.html"><meta name="DC.Relation" scheme="URI" content="../topics/cr-hive_convert_an_hdfs_file_to_orc.html"><meta name="DC.Relation" scheme="URI" content="https://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html"><meta name="prodname" content="Cloudera Runtime"><meta name="version" content="1"><meta name="release" content="0"><meta name="modification" content="0"><meta name="brand" content="Cloudera Runtime"><meta name="rights" content="© 2019 Cloudera, Inc."><meta name="DC.Date.Created" content="2019-04-15"><meta name="DC.Date.Modified" content="2019-04-15"><meta name="DC.Format" content="XHTML"><meta name="DC.Identifier" content="hive_import_rdbms_data_for_querying_in_hive"><link rel="stylesheet" type="text/css" href="../commonltr.css"><title>Import RDBMS data to HDFS</title><script type="application/javascript">
      
      
    //The id for tree cookie
    var treeCookieId = "treeview-bk_release-notes";
    var language = "en";
    var w = new Object();
    //Localization
    txt_filesfound = 'Results';
    txt_enter_at_least_1_char = "You must enter at least one character.";
    txt_browser_not_supported = "Please enable JavaScript.";
    txt_please_wait = "Please wait. Search in progress...";
    txt_results_for = "Results for: ";
    
    
    </script><script type="application/javascript" src="https://code.jquery.com/jquery-1.4.3.min.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/jquery-ui-1.8.2.custom.min.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/jquery.cookie.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/jquery.treeview.min.js"></script><link href="https://cdn.jsdelivr.net/qtip2/3.0.3/jquery.qtip.min.css" rel="stylesheet" type="text/css" media="screen, projection"><script src="https://cdnjs.cloudflare.com/ajax/libs/qtip2/3.0.3/jquery.qtip.min.js" type="application/javascript"></script><script type="application/javascript" src="/common/js/navigation.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/main.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/htmlFileList.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/htmlFileInfoList.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/nwSearchFnt.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/en_stemmer.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/index-1.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/index-2.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/index-3.js"></script><script type="application/javascript">
      
      
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-22950817-19', 'auto');
    ga('send', 'pageview');
    
    
    </script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/plugins.js"></script><script type="application/javascript" src="/common/themes/pre-hdp-3.1/js/scripts.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/highlight.min.js" type="application/javascript"></script><script type="application/javascript">
      
    // PAGE SPECIFIC SCRIPTS
    
    $('.chapters>li .dropdown').click(function () {
    var $arrow = $(this).find('i.fa');
    if( $arrow.hasClass('fa-chevron-down')) {
    $(this).siblings('.sections').show();
    $arrow.removeClass('fa-chevron-down').addClass('fa-chevron-up');
    } else {
    console.log('up');
    $(this).siblings('.sections').hide();
    $arrow.removeClass('fa-chevron-up').addClass('fa-chevron-down');
    }
    });
    
    //  $('h3').on('mouseenter',function() {
    //      console.log('received mouseover');
    //      $(this).find('.hash-link').show();
    //  });
    
    //  $('h3').on('mouseleave',function() {
    //      $(this).find('.hash-link').hide();
    //  });
    
    
    // STICKY SIDEBARs
    var stickySidebar = $('.sticky');
    
    if (stickySidebar.length > 0) {
    var stickyHeight = stickySidebar.height(),
    sidebarTop = stickySidebar.offset().top;
    }
    
    // on scroll move the sidebar
    $(window).scroll(function () {
    if (stickySidebar.length > 0) {
    var scrollTop = $(window).scrollTop();
    console.log
    
    if (sidebarTop < scrollTop) {
    stickySidebar.css('top', scrollTop - sidebarTop);
    
    // stop the sticky sidebar at the footer to avoid overlapping
    var sidebarBottom = stickySidebar.offset().top + stickyHeight,
    stickyStop = $('.main-content').offset().top + $('.main-content').height();
    if (stickyStop < sidebarBottom) {
    var stopPosition = $('.main-content').height() - stickyHeight;
    stickySidebar.css('top', stopPosition);
    }
    }
    else {
    stickySidebar.css('top', '0');
    }
    }
    });
    
    $(window).resize(function () {
    if (stickySidebar.length > 0) {
    stickyHeight = stickySidebar.height();
    }
    });
    
    // Initialize Syntax Highlighting
    hljs.initHighlightingOnLoad();
    
    </script><style type="text/css">
      
      
    input {
    margin-bottom: 5px;
    margin-top: 2px;
    }
    
    .folder {
    display: block;
    height: 22px;
    padding-left: 20px;
    background: transparent url(/common/themes/pre-hdp-3.1/images/folder.gif) 0 0px no-repeat;
    }
    
    </style><link href="/common/themes/pre-hdp-3.1/images/favicon.ico" rel="icon" type="image/x-icon" sizes="16x16"><link href="https://hortonworks.com/wp-content/uploads/2016/04/cropped-favicon521x512-70x70.png" rel="icon" type="image/x-icon" sizes="32x32"><link href="https://hortonworks.com/wp-content/uploads/2016/04/cropped-favicon521x512-300x300.png" rel="icon" type="image/x-icon" sizes="192x192"><link href="https://hortonworks.com/wp-content/uploads/2016/04/cropped-favicon521x512-300x300.png" rel="apple-touch-icon-precomposed" type="image/x-icon" sizes="192x192"><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/styles/default.min.css" rel="stylesheet"><link rel="stylesheet" media="screen, projection" href="/common/themes/pre-hdp-3.1/css/main.css"><link rel="stylesheet" media="screen, projection" href="/common/themes/pre-hdp-3.1/css/docux.css"><style type="text/css"></style><link rel="stylesheet" type="text/css" media="screen, projection" href="/common/themes/pre-hdp-3.1/css/jquery-ui-1.8.2.custom.css"><link rel="stylesheet" type="text/css" media="screen, projection" href="/common/themes/pre-hdp-3.1/css/jquery.treeview.css"><link href="https://cdn.jsdelivr.net/qtip2/3.0.3/jquery.qtip.min.css" rel="stylesheet" type="text/css" media="screen, projection"></head><body class="docs books"><div id="body_container"><div class="breadcrumb_bg"><!-- --></div><div id="navbar"><section class="ps topstrip"><div><!-- --></div></section></div><div id="mainbody"><section class="ps titlebar"><div><div class="breadcrumb"><a href="/">Hortonworks Docs</a> » <a id="index_page" href="../../index.html">Cloudera Runtime 1.0.0</a> » <span class="current_book">Migrating Data</span></div><div class="book_title">Migrating Data</div><div class="ebooks"><div class="formats"><span class="desc">Also available as:</span><div class="format"><a onclick="_gaq.push(['_trackEvent', 'Header', 'pdfDownload', 'click', 1]);" title="Download a PDF of this document" class="pdficon" href="../cr-hive_migrating_data.pdf"><img src="/common/themes/pre-hdp-3.1/images/pdf.png" alt="PDF"></a></div></div></div></div></section><div class="lowerbody"><section class="ps doc-content"><div><div class="aside left"><div><div id="leftnavigation" style="padding-top:3px; background-color:white;"><div class="ui-tabs ui-widget ui-widget-content ui-corner-all"><div id="treeDiv" class="ui-tabs-panel ui-widget-content ui-corner-bottom"><img src="/common/themes/pre-hdp-3.1/images/loading.gif" alt="loading table of contents..." id="tocLoading" style="display:block;"><div id="ulTreeDiv" style="display:none" class="thisisthat"><ul id="tree" class="filetree"><li><a href="../topics/cr-hive_data_migration.html">Data migration to Apache Hive</a></li><li><a href="../topics/cr-hive_moving_data_from_databases_to_hive.html">Moving data from databases to Apache Hive</a><ul><li><a href="../topics/cr-hive_create_a_sqoop_import_command.html">Create a Sqoop import command</a></li><li><a href="../topics/cr-hive_import_rdbms_data_into_hive.html">Import RDBMS data into Hive</a></li></ul></li><li><a href="../topics/cr-hive_moving_data_from_hdfs_to_hive.html">Moving data from HDFS to Apache Hive</a><ul><li class="active"><a href="../topics/cr-hive_import_rdbms_data_to_hdfs.html">Import RDBMS data to HDFS</a></li><li><a href="../topics/cr-hive_convert_an_hdfs_file_to_orc.html">Convert an HDFS file to ORC</a></li><li><a href="../topics/cr-hive_incrementally_update_an_imported_table.html">Incrementally update an imported table</a></li></ul></li><li><a href="../topics/cr-hive_import_command_options.html">Hive import command options</a></li></ul></div></div></div></div></div><div class="formats"><div class="pdf"><!-- --></div></div></div><div class="next-prev-links"><span class="nextlink"><a class="link" href="../topics/cr-hive_convert_an_hdfs_file_to_orc.html" title="To query data in HDFS in Hive, you apply a schema to the data and then store data in ORC format.">Next</a></span></div><div id="content" aria-labelledby="ariaid-title1">
   <h1 class="title topictitle1" id="ariaid-title1">Import RDBMS data to
      HDFS</h1>
   
   <div class="body taskbody"><p class="shortdesc">You create a single
      Sqoop import command that imports data from diverse data sources, such as a relational
      database, into HDFS.</p>
      <section class="section context">You enter the Sqoop import command on the command line of your cluster to import data
         from a data source into HDFS. In HDFS, you can perform ETL on the data, move the data into
         Hive, and query the data. The import command needs to include the database URI, database
         name, and connection protocol, such as <code class="ph codeph">jdbc:mysql:</code> and the data to import.
         Optionally, the command can include parallel processing directives for performant data
         transfer, the HDFS destination directory for imported data, data delimiters, and other
         information. The default directory is used if you do not specify another location. Fields
         are comma-delimited and rows are line-delimited.You can test the import statement before
         actually executing it. </section>
      <div class="section prereq p"><ul class="ul">
         <li class="li">Apache Sqoop is installed and configured.</li>
      </ul></div>
      <ol class="ol steps"><li class="li step stepexpand">
            <span class="ph cmd">Create an import command that specifies the Sqoop connection to the data source you
               want to import.</span>
            <ul class="ul choices">
               <li class="li choice">If you want to enter a password for the data source on the command line, use
                  the <code class="ph codeph">-P</code> option in the connection string.</li>
               <li class="li choice">If you want to specify a file where the password is stored, use the
                     <code class="ph codeph">--password-file</code> option.</li>
            </ul>
            <div class="itemgroup stepxmp">Password on command line:
               <pre class="pre codeblock"><code>sqoop import --connect jdbc:mysql://db.foo.com/bar \
&lt;data to import&gt; \
--username &lt;username&gt; \
-P</code></pre></div>
            <div class="itemgroup stepxmp">Specify password file:
               <pre class="pre codeblock"><code>sqoop import --connect jdbc:mysql://db.foo.com/bar \
--table EMPLOYEES \
--username &lt;username&gt; \
--password-file ${user.home}/.password</code></pre></div>
         </li><li class="li step stepexpand">
            <span class="ph cmd">Specify the data to import in the command.</span>
            <ul class="ul choices">
               <li class="li choice">Import an entire table.</li>
               <li class="li choice">Import a subset of the columns.</li>
               <li class="li choice">Import data using a free-form query.</li>
            </ul>
            <div class="itemgroup stepxmp">Entire table:
               <pre class="pre codeblock"><code>sqoop import \
--connect jdbc:mysql://db.foo.com/bar \
--table EMPLOYEES</code></pre></div>
            <div class="itemgroup stepxmp">Subset of columns:
               <pre class="pre codeblock"><code>sqoop import \
--connect jdbc:mysql://db.foo.com/bar \
--table EMPLOYEES \
--columns "employee_id,first_name,last_name,job_title"</code></pre></div>
            <div class="itemgroup stepxmp">Free-form query to import the latest data:
               <pre class="pre codeblock"><code>sqoop import \
--connect jdbc:mysql://db.foo.com/bar \
--table EMPLOYEES \
--where "start_date &gt; '2018-01-01'"</code></pre>
            </div>
         </li><li class="li step stepexpand">
            <span class="ph cmd">Specify the destination of the imported data using the
                  <code class="ph codeph">--target-dir</code> option.</span>
            <div class="itemgroup stepxmp">This command appends data imported from the MySQL EMPLOYEES table to the output
               files in the HDFS target directory using default text file delimiters.
               <pre class="pre codeblock"><code>sqoop import \
--connect jdbc:mysql://db.foo.com:3600/bar \
--table EMPLOYEES \
--where "id &gt; 100000" \
--target-dir /incremental_dataset \
--append</code></pre></div>
            <div class="itemgroup stepxmp">This command splits imported data by column and specifies importing the data
               into output files in the HDFS target directory.
               <pre class="pre codeblock"><code>sqoop import \
--connect jdbc:mysql://db.foo.com:3600/bar \
--query 'SELECT a.*, b.* \
FROM a JOIN b on (a.id == b.id) \
WHERE $CONDITIONS' \
--split-by a.id \
--target-dir /user/foo/joinresults</code></pre></div>
            <div class="itemgroup stepxmp">This command executes once and imports data serially using a single map task as
               specified by the -m 1 options:
               <pre class="pre codeblock"><code>sqoop import \
--connect jdbc:mysql://db.foo.com:3600/bar \
--query \
'SELECT a.*, b.* \
FROM a \
JOIN b on (a.id == b.id) \
WHERE $CONDITIONS' \
-m 1 \
--target-dir /user/foo/joinresults</code></pre></div>
         </li><li class="li step stepexpand">
            <span class="ph cmd">Optionally, specify <em class="ph i">write parallelism</em> in the import statement to execute a
               number of map tasks in parallel:</span>
            <ul class="ul choices">
               <li class="li choice">Set mappers: If the source table has a primary key, explicitly set the number
                  of mappers using <code class="ph codeph">--num-mappers</code>.</li>
               <li class="li choice">Split by: If primary keys are not evenly distributed, provide a split key
                  using <code class="ph codeph">--split-by</code></li>
               <li class="li choice">Sequential: If you do not have a primary key or split key, import data
                  sequentially using <code class="ph codeph">--num-mappers 1</code> or
                     <code class="ph codeph">--autoreset-to-one-mapper</code> in query.</li>
            </ul>
            <div class="itemgroup stepxmp">
               <ul class="ul">
                  <li class="li">Set mappers:
                     <pre class="pre codeblock"><code>sqoop import --connect jdbc:mysql://db.foo.com:3306/bar \
--table EMPLOYEES \
--num-mappers 8</code></pre></li>
                  <li class="li">Split by:
                     <pre class="pre codeblock"><code>sqoop import --connect jdbc:mysql://db.foo.com:3306/bar \
--table EMPLOYEES \
--split-by dept_id</code></pre></li>
               </ul>
            </div>
            <div class="itemgroup stepresult">
               <ul class="ul">
                  <li class="li">Setting mappers evenly splits the primary key range of the source table.</li>
                  <li class="li">Split by evenly splits the data using the split key instead of a primary
                     key.</li>
               </ul>
            </div>
         </li><li class="li step stepexpand">
            <span class="ph cmd">Optionally, test the import command before execution using the eval option.</span>
            <div class="itemgroup stepxmp">
               <pre class="pre codeblock"><code>sqoop eval --connect jdbc:mysql://db.foo.com:3306/bar \
--query "SELECT * FROM employees LIMIT 10"</code></pre>
            </div>
            <div class="itemgroup stepresult">The output of the select statement appears.</div>
         </li></ol>
              
                
               
     </div>
<nav role="navigation" class="related-links"><div class="familylinks"><div class="parentlink"><strong>Parent topic:</strong> <a class="link" href="../topics/cr-hive_moving_data_from_hdfs_to_hive.html" title="You can import data from diverse data sources into HDFS, perform ETL processes, and then query the data in Apache Hive.">Moving data from HDFS to Apache Hive</a></div></div><div class="linklist relinfo"><strong>Related information</strong><br><div><a class="link" href="https://sqoop.apache.org/docs/1.4.7/SqoopUserGuide.html" target="_blank">Sqoop User Guide</a></div></div></nav></div></div></section></div></div></div><section class="ps footer"><div><div class="copyright">© 2012–2019, Hortonworks, Inc.</div><div class="license">Document licensed under the <a href="https://creativecommons.org/licenses/by-sa/4.0/legalcode">Creative
          Commons Attribution ShareAlike 4.0 License</a>.</div><div class="menulinks"><a href="https://hortonworks.com">Hortonworks.com</a> | 
          <a href="#">Documentation</a> |
          <a href="https://hortonworks.com/support/">Support</a> |
          <a href="https://community.hortonworks.com/">Community</a></div></div></section></body></html>